{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speech-to-LaTeX demo\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dkorzh10/speech2latex/blob/main/demo_speech2latex.ipynb)\n",
        "\n",
        "Try ASR post-correction models: play samples from the repo or record your own audio (Colab microphone / file upload)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install and clone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate torch torchaudio huggingface_hub peft whisper ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "REPO_DIR = \"speech2latex\"\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone --depth 1 https://github.com/dkorzh10/speech2latex.git\n",
        "os.chdir(REPO_DIR)\n",
        "asr_path = os.path.join(os.getcwd(), \"ASRPostCorrection\")\n",
        "if asr_path not in sys.path:\n",
        "    sys.path.insert(0, asr_path)\n",
        "print(\"Repo root:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model choice and load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Available models on Hugging Face (marsianin500)\n",
        "MODELS = [\n",
        "    \"marsianin500/Qwen2.5-0.5B-instruct-equations_multilingual_mix\",\n",
        "    \"marsianin500/Qwen2.5-0.5B-instruct-equations_multilingual_mix_full\",\n",
        "    \"marsianin500/Qwen2.5-0.5B-instruct-sentences_eng_mix\",\n",
        "    \"marsianin500/Qwen2.5-1.5B-instruct-equations_multilingual_mix\",\n",
        "    \"marsianin500/Qwen2.5-math-1.5B-instruct-equations_multilingual_mix_full\",\n",
        "    \"marsianin500/Qwen2.5-math-1.5B-instruct-equations_multilingual_mix\",\n",
        "    \"marsianin500/Qwen2.5-math-1.5B-instruct-sentences_eng_mix\",\n",
        "    \"marsianin500/Qwen2.5-7B-instruct-r16a64-equations_multilingual_mix\",\n",
        "    \"marsianin500/Qwen2.5-7B-instruct-r16a64-equations_multilingual_mix_full\",\n",
        "]\n",
        "\n",
        "# 7B models are LoRA adapters; base model name for them\n",
        "BASE_7B = \"Qwen/Qwen2.5-7B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[(m.replace(\"marsianin500/\", \"\")[:70], m) for m in MODELS],\n",
        "    value=MODELS[0],\n",
        "    description=\"Model:\",\n",
        "    layout=widgets.Layout(width=\"500px\")\n",
        ")\n",
        "display(model_dropdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "def load_model(repo_id: str, device: str = \"cuda\"):\n",
        "    from huggingface_hub import list_repo_files\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "    except Exception:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(repo_id, subfolder=\"tokenizer\")\n",
        "    # Check if repo has tuned-model (full) or adapter (LoRA)\n",
        "    files = list_repo_files(repo_id)\n",
        "    has_tuned = any(\"tuned-model\" in f or f.startswith(\"tuned-model/\") for f in files)\n",
        "    has_adapter = any(\"adapter_model\" in f or \"adapter_config\" in f for f in files)\n",
        "    if has_tuned:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            repo_id, subfolder=\"tuned-model\",\n",
        "            torch_dtype=torch.bfloat16, device_map=device\n",
        "        )\n",
        "    elif has_adapter:\n",
        "        base = AutoModelForCausalLM.from_pretrained(\n",
        "            BASE_7B, torch_dtype=torch.bfloat16, device_map=device\n",
        "        )\n",
        "        model = PeftModel.from_pretrained(base, repo_id)\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            repo_id, torch_dtype=torch.bfloat16, device_map=device\n",
        "        )\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Loading {model_dropdown.value} on {device}...\")\n",
        "tokenizer, model = load_model(model_dropdown.value, device)\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Inference helper (from repo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, os.path.join(os.getcwd(), \"ASRPostCorrection\"))\n",
        "from chat_template_with_generation import CHAT_TEMPLATE_WITH_GENERATION\n",
        "\n",
        "def transcribe_whisper(audio_path_or_array, language=\"en\"):\n",
        "    import whisper\n",
        "    w = whisper.load_model(\"base\", device=device)\n",
        "    if isinstance(audio_path_or_array, str):\n",
        "        r = w.transcribe(audio_path_or_array, language=language, fp16=(device==\"cuda\"))\n",
        "    else:\n",
        "        r = w.transcribe(audio_path_or_array, language=language, fp16=(device==\"cuda\"))\n",
        "    return (r.get(\"text\") or \"\").strip()\n",
        "\n",
        "def pronunciation_to_latex(pronunciation: str):\n",
        "    chat = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Please, give me LaTeX representation of the following formula. Formula pronunciation: \" + pronunciation},\n",
        "    ]\n",
        "    out = tokenizer.apply_chat_template(\n",
        "        chat, padding=True, tokenize=True, chat_template=CHAT_TEMPLATE_WITH_GENERATION,\n",
        "        return_assistant_tokens_mask=True, return_dict=True, return_tensors=\"pt\",\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    gen_ids = model.generate(\n",
        "        inputs=out[\"input_ids\"].to(model.device),\n",
        "        attention_mask=out[\"attention_mask\"].to(model.device),\n",
        "        max_new_tokens=256, do_sample=False, pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    text = tokenizer.decode(gen_ids[0][out[\"input_ids\"].shape[1]:], skip_special_tokens=False)\n",
        "    if \"<|im_end|>\" in text: text = text.split(\"<|im_end|>\")[0]\n",
        "    if \"<|endoftext|>\" in text: text = text.split(\"<|endoftext|>\")[0]\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Demo: samples from repo (no dataset download)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GITHUB_RAW = \"https://raw.githubusercontent.com/dkorzh10/speech2latex/main\"\n",
        "SAMPLES = {\n",
        "    \"equations_test\": [\"human_eng_00\", \"human_eng_01\", \"human_eng_02\", \"human_eng_03\", \"tts_eng_00\"],\n",
        "    \"sentences_test\": [\"human_eng_00\", \"tts_eng_00\"],\n",
        "}\n",
        "\n",
        "import urllib.request\n",
        "import json\n",
        "\n",
        "# Optional: load precomputed demo_results.json for reference/predicted\n",
        "demo_results = {}\n",
        "try:\n",
        "    with urllib.request.urlopen(GITHUB_RAW + \"/docs/demo_results.json\") as r:\n",
        "        data = json.load(r)\n",
        "        for item in data.get(\"results\", []):\n",
        "            key = item[\"split\"] + \"/\" + item[\"sample_id\"]\n",
        "            demo_results[key] = item\n",
        "except Exception as e:\n",
        "    print(\"Could not load demo_results.json:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "split = \"equations_test\"\n",
        "sample_id = \"human_eng_01\"\n",
        "url = f\"{GITHUB_RAW}/sample_datasets/{split}/{sample_id}.wav\"\n",
        "local_wav = \"/tmp/sample.wav\"\n",
        "urllib.request.urlretrieve(url, local_wav)\n",
        "\n",
        "from IPython.display import Audio, display\n",
        "display(Audio(local_wav))\n",
        "\n",
        "pron = transcribe_whisper(local_wav, language=\"en\")\n",
        "print(\"Whisper:\", pron)\n",
        "latex = pronunciation_to_latex(pron)\n",
        "print(\"LaTeX:\", latex)\n",
        "key = split + \"/\" + sample_id\n",
        "if key in demo_results:\n",
        "    print(\"Reference:\", demo_results[key].get(\"reference_latex\", \"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Record your own audio (microphone) or upload file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "print(\"Upload an audio file (e.g. .wav) or use the recorder below.\")\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "    fname = list(uploaded.keys())[0]\n",
        "    user_audio_path = f\"/tmp/user_{fname}\"\n",
        "    with open(user_audio_path, \"wb\") as f:\n",
        "        f.write(uploaded[fname])\n",
        "    print(\"Saved to\", user_audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: record from microphone (browser). Click Record, speak, Stop; then upload the downloaded file above.\n",
        "from IPython.display import HTML, display\n",
        "display(HTML(\"\"\"\n",
        "<button id=\"recBtn\">Record</button>\n",
        "<span id=\"status\"></span>\n",
        "<script>\n",
        "let rec, chunks = [];\n",
        "document.getElementById('recBtn').onclick = async function() {\n",
        "  if (!rec || rec.state === 'inactive') {\n",
        "    const s = await navigator.mediaDevices.getUserMedia({audio:true});\n",
        "    rec = new MediaRecorder(s);\n",
        "    chunks = []; rec.ondataavailable = e => chunks.push(e.data);\n",
        "    rec.onstop = () => { const a = document.createElement('a'); a.href = URL.createObjectURL(new Blob(chunks, {type:'audio/webm'})); a.download = 'recording.webm'; a.click(); document.getElementById('status').textContent = 'Downloaded recording.webm â€” upload it above.'; };\n",
        "    rec.start(); this.textContent = 'Stop'; document.getElementById('status').textContent = 'Recording...';\n",
        "  } else { rec.stop(); this.textContent = 'Record'; }\n",
        "};\n",
        "</script>\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process uploaded/recorded audio (use the file you uploaded in the cell above)\n",
        "try: _ = uploaded\n",
        "except NameError: uploaded = {}\n",
        "user_audio_path = (\"/tmp/user_\" + list(uploaded.keys())[0]) if uploaded else None\n",
        "if user_audio_path and os.path.isfile(user_audio_path):\n",
        "    display(Audio(user_audio_path))\n",
        "    lang = \"en\"  # or \"ru\" for Russian\n",
        "    pron = transcribe_whisper(user_audio_path, language=lang)\n",
        "    print(\"Whisper:\", pron)\n",
        "    latex = pronunciation_to_latex(pron)\n",
        "    print(\"LaTeX:\", latex)\n",
        "else:\n",
        "    print(\"Upload an audio file (.wav or .webm) in the cell above, then re-run that cell and this one.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
